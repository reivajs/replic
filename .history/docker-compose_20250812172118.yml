# =====================================================
# DOCKER COMPOSE - MICROSERVICES ORCHESTRATION
# =====================================================
# docker-compose.yml

version: '3.8'

services:
  # ========== DATABASES ==========
  postgres:
    image: postgres:15-alpine
    container_name: replication_postgres
    environment:
      POSTGRES_USER: replication_user
      POSTGRES_PASSWORD: secure_password_here
      POSTGRES_DB: replication_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - replication_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U replication_user"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: replication_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - replication_network
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ========== MICROSERVICES ==========
  
  # Account Service - Multi-tenant management
  account-service:
    build:
      context: ./services/account_service
      dockerfile: Dockerfile
    container_name: account_service
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://replication_user:secure_password_here@postgres:5432/replication_db
      - REDIS_URL=redis://redis:6379
      - SERVICE_NAME=account-service
      - LOG_LEVEL=INFO
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./services/account_service:/app
      - ./sessions:/app/sessions
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Replication Engine - Core replication logic
  replication-engine:
    build:
      context: ./services/replication_engine
      dockerfile: Dockerfile
    container_name: replication_engine
    ports:
      - "8001:8001"
    environment:
      - DATABASE_URL=postgresql://replication_user:secure_password_here@postgres:5432/replication_db
      - REDIS_URL=redis://redis:6379
      - SERVICE_NAME=replication-engine
      - LOG_LEVEL=INFO
      - MAX_WORKERS=10
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./services/replication_engine:/app
      - ./media:/app/media
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Discovery Service - Channel discovery
  discovery-service:
    build:
      context: ./services/discovery_service
      dockerfile: Dockerfile
    container_name: discovery_service
    ports:
      - "8002:8002"
    environment:
      - DATABASE_URL=postgresql://replication_user:secure_password_here@postgres:5432/replication_db
      - REDIS_URL=redis://redis:6379
      - SERVICE_NAME=discovery-service
      - LOG_LEVEL=INFO
      - SCAN_INTERVAL=1800
      - RATE_LIMIT_DELAY=1.0
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./services/discovery_service:/app
      - ./sessions:/app/sessions
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring Service - Real-time monitoring
  monitoring-service:
    build:
      context: ./services/monitoring_service
      dockerfile: Dockerfile
    container_name: monitoring_service
    ports:
      - "8003:8003"
      - "8080:8080"  # WebSocket port
    environment:
      - REDIS_URL=redis://redis:6379
      - SERVICE_NAME=monitoring-service
      - LOG_LEVEL=INFO
      - METRICS_INTERVAL=5
    depends_on:
      - redis
      - account-service
      - replication-engine
      - discovery-service
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./services/monitoring_service:/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Watermark Service - Image processing
  watermark-service:
    build:
      context: ./services/watermark_service
      dockerfile: Dockerfile
    container_name: watermark_service
    ports:
      - "8004:8004"
    environment:
      - SERVICE_NAME=watermark-service
      - LOG_LEVEL=INFO
      - MAX_FILE_SIZE_MB=50
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./services/watermark_service:/app
      - ./media:/app/media
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== FRONTEND ==========
  
  # Control Center UI
  control-center:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: control_center
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_BASE_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8003/ws
    depends_on:
      - account-service
      - replication-engine
      - discovery-service
      - monitoring-service
    networks:
      - replication_network
    restart: unless-stopped
    volumes:
      - ./frontend:/app
      - /app/node_modules

  # ========== REVERSE PROXY ==========
  
  nginx:
    image: nginx:alpine
    container_name: replication_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - control-center
      - account-service
      - replication-engine
      - discovery-service
      - monitoring-service
    networks:
      - replication_network
    restart: unless-stopped

networks:
  replication_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:

---
# =====================================================
# DOCKERFILE - PYTHON MICROSERVICES
# =====================================================
# services/[service_name]/Dockerfile

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Start application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]

---
# =====================================================
# DOCKERFILE - FRONTEND (REACT)
# =====================================================
# frontend/Dockerfile

FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci

# Copy source code
COPY . .

# Build application
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built files
COPY --from=builder /app/build /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port
EXPOSE 3000

# Start nginx
CMD ["nginx", "-g", "daemon off;"]

---
# =====================================================
# NGINX CONFIGURATION
# =====================================================
# nginx/nginx.conf

events {
    worker_connections 1024;
}

http {
    upstream account_service {
        server account-service:8000;
    }

    upstream replication_engine {
        server replication-engine:8001;
    }

    upstream discovery_service {
        server discovery-service:8002;
    }

    upstream monitoring_service {
        server monitoring-service:8003;
    }

    upstream control_center {
        server control-center:3000;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/s;
    limit_conn_zone $binary_remote_addr zone=addr:10m;

    server {
        listen 80;
        server_name localhost;

        # Security headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;

        # Frontend
        location / {
            proxy_pass http://control_center;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Account Service API
        location /api/accounts/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://account_service/api/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Replication Engine API
        location /api/replication/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://replication_engine/api/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Discovery Service API
        location /api/discovery/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://discovery_service/api/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Monitoring WebSocket
        location /ws {
            proxy_pass http://monitoring_service/ws;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Health checks
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}

---
# =====================================================
# KUBERNETES DEPLOYMENT (OPTIONAL)
# =====================================================
# k8s/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: account-service
  namespace: replication-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: account-service
  template:
    metadata:
      labels:
        app: account-service
    spec:
      containers:
      - name: account-service
        image: replication/account-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: account-service
  namespace: replication-system
spec:
  selector:
    app: account-service
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: account-service-hpa
  namespace: replication-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: account-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

---
# =====================================================
# ENVIRONMENT VARIABLES (.env)
# =====================================================
# .env.production

# Database
DATABASE_URL=postgresql://replication_user:secure_password@localhost:5432/replication_db
REDIS_URL=redis://localhost:6379

# Services URLs
ACCOUNT_SERVICE_URL=http://localhost:8000
REPLICATION_ENGINE_URL=http://localhost:8001
DISCOVERY_SERVICE_URL=http://localhost:8002
MONITORING_SERVICE_URL=http://localhost:8003
WATERMARK_SERVICE_URL=http://localhost:8004

# Security
JWT_SECRET_KEY=your-super-secret-jwt-key-change-this
API_KEY=your-api-key-for-internal-communication
ENCRYPTION_KEY=your-encryption-key-for-sensitive-data

# Telegram (Default - each tenant will have their own)
DEFAULT_TELEGRAM_API_ID=your_api_id
DEFAULT_TELEGRAM_API_HASH=your_api_hash

# Monitoring
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
PROMETHEUS_ENABLED=true
GRAFANA_API_KEY=your-grafana-api-key

# Storage
S3_BUCKET_NAME=replication-media
S3_ACCESS_KEY=your-s3-access-key
S3_SECRET_KEY=your-s3-secret-key
S3_REGION=us-east-1

# Rate Limiting
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_PER_HOUR=1000

# Scaling
MAX_WORKERS_PER_TENANT=10
MAX_CONCURRENT_FLOWS=50
MAX_MESSAGE_QUEUE_SIZE=10000

---
# =====================================================
# REQUIREMENTS.TXT - PYTHON DEPENDENCIES
# =====================================================
# services/requirements.txt

# Core
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-dotenv==1.0.0

# Database
sqlalchemy==2.0.23
alembic==1.12.1
asyncpg==0.29.0
psycopg2-binary==2.9.9

# Redis
redis==5.0.1
aioredis==2.0.1

# Telegram
telethon==1.32.0
cryptg==0.4.0
hachoir==3.2.0

# Discord
aiohttp==3.9.1
discord.py==2.3.2

# Image Processing
Pillow==10.1.0
opencv-python==4.8.1.78

# Authentication & Security
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6

# Monitoring
prometheus-client==0.19.0
sentry-sdk==1.38.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2

# Utils
python-dateutil==2.8.2
pytz==2023.3
tenacity==8.2.3
loguru==0.7.2

---
# =====================================================
# MAKEFILE - BUILD AUTOMATION
# =====================================================
# Makefile

.PHONY: help build up down restart logs clean test deploy

help:
	@echo "Available commands:"
	@echo "  make build    - Build all Docker images"
	@echo "  make up       - Start all services"
	@echo "  make down     - Stop all services"
	@echo "  make restart  - Restart all services"
	@echo "  make logs     - Show logs from all services"
	@echo "  make clean    - Clean up containers and volumes"
	@echo "  make test     - Run tests"
	@echo "  make deploy   - Deploy to production"

build:
	docker-compose build

up:
	docker-compose up -d
	@echo "Services started. Access Control Center at http://localhost:3000"

down:
	docker-compose down

restart:
	docker-compose restart

logs:
	docker-compose logs -f

clean:
	docker-compose down -v
	docker system prune -f

test:
	docker-compose run --rm account-service pytest
	docker-compose run --rm replication-engine pytest
	docker-compose run --rm discovery-service pytest

deploy:
	@echo "Deploying to production..."
	kubectl apply -f k8s/
	@echo "Deployment complete!"

# Development commands
dev-setup:
	cp .env.example .env
	make build
	make up

dev-reset:
	make clean
	make dev-setup

# Database migrations
db-migrate:
	docker-compose exec account-service alembic upgrade head

db-rollback:
	docker-compose exec account-service alembic downgrade -1

# Monitoring
monitor:
	open http://localhost:3000/monitoring
	docker-compose logs -f monitoring-service

# Backup
backup:
	docker-compose exec postgres pg_dump -U replication_user replication_db > backup_$(date +%Y%m%d_%H%M%S).sql

restore:
	docker-compose exec -T postgres psql -U replication_user replication_db < $(FILE)