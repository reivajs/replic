"""
üîß FILE CACHE MEJORADO - SOLUCI√ìN ENTERPRISE
============================================
Reemplazo directo para la clase FileCache en app/services/file_processor.py
Soluciona el error de JSON con manejo robusto de Unicode y recuperaci√≥n autom√°tica
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
import hashlib
import shutil

logger = logging.getLogger(__name__)

class FileCache:
    """
    Cache inteligente para archivos procesados con manejo robusto de errores
    Soluciona el problema de caracteres Unicode y JSON corrupto
    """
    
    def __init__(self, cache_dir: Path, max_cache_size_gb: float = 2.0):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_cache_size = max_cache_size_gb * 1024 * 1024 * 1024  # GB to bytes
        self.cache_index_file = cache_dir / "cache_index.json"
        self.backup_dir = cache_dir / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        self.cache_index = self._load_cache_index()
    
    def _load_cache_index(self) -> Dict[str, Any]:
        """Cargar √≠ndice de cache con recuperaci√≥n autom√°tica"""
        if not self.cache_index_file.exists():
            logger.info("üìÇ No existe cache_index.json, creando nuevo...")
            return self._create_empty_cache()
        
        try:
            with open(self.cache_index_file, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
                
                if not content.strip():
                    logger.warning("‚ö†Ô∏è Cache index vac√≠o, inicializando nuevo cache")
                    return self._create_empty_cache()
                
                try:
                    data = json.loads(content)
                    logger.info("‚úÖ Cache index cargado correctamente")
                    return self._sanitize_cache_data(data)
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"‚ùå Error decodificando JSON: {e}")
                    return self._recover_corrupted_cache(content, e)
                    
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico cargando cache: {e}")
            return self._handle_critical_error()
    
    def _create_empty_cache(self) -> Dict[str, Any]:
        """Crear estructura de cache vac√≠a"""
        return {
            "version": "2.0",
            "created": datetime.now().isoformat(),
            "last_modified": datetime.now().isoformat(),
            "entries": {},
            "metadata": {
                "total_files": 0,
                "total_size_bytes": 0,
                "recovery_count": 0
            }
        }
    
    def _sanitize_cache_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitizar datos del cache para asegurar compatibilidad"""
        if "version" not in data:
            logger.info("üì¶ Migrando cache de formato antiguo a nuevo...")
            new_cache = self._create_empty_cache()
            new_cache["entries"] = data
            new_cache["metadata"]["migrated_from_v1"] = datetime.now().isoformat()
            return new_cache
        
        if "entries" not in data:
            data["entries"] = {}
        if "metadata" not in data:
            data["metadata"] = {}
            
        return data
    
    def _recover_corrupted_cache(self, content: str, error: json.JSONDecodeError) -> Dict[str, Any]:
        """Intentar recuperar datos de un cache corrupto"""
        logger.info("üîß Intentando recuperar datos del cache corrupto...")
        
        self._backup_corrupted_file(content)
        
        try:
            error_pos = error.pos if hasattr(error, 'pos') else len(content)
            
            for i in range(error_pos - 1, max(0, error_pos - 1000), -1):
                partial_content = content[:i]
                
                if partial_content.rstrip().endswith('}'):
                    try:
                        recovered_data = json.loads(partial_content)
                        logger.info(f"‚úÖ Recuperados {len(recovered_data)} entries del cache")
                        
                        sanitized = self._sanitize_cache_data(recovered_data)
                        sanitized["metadata"]["recovered_at"] = datetime.now().isoformat()
                        sanitized["metadata"]["recovery_count"] =                             sanitized["metadata"].get("recovery_count", 0) + 1
                        
                        return sanitized
                        
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            logger.error(f"‚ùå Error en recuperaci√≥n parcial: {e}")
        
        logger.warning("‚ö†Ô∏è No se pudo recuperar el cache, creando uno nuevo")
        new_cache = self._create_empty_cache()
        new_cache["metadata"]["corruption_detected"] = datetime.now().isoformat()
        return new_cache
    
    def _backup_corrupted_file(self, content: str):
        """Crear backup del archivo corrupto"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"cache_index_corrupted_{timestamp}.json.bak"
            
            with open(backup_file, 'w', encoding='utf-8', errors='replace') as f:
                f.write(content)
            
            logger.info(f"üíæ Backup del cache corrupto guardado en: {backup_file}")
            
        except Exception as e:
            logger.error(f"‚ùå Error creando backup: {e}")
    
    def _handle_critical_error(self) -> Dict[str, Any]:
        """Manejar errores cr√≠ticos creando un cache limpio"""
        logger.error("üÜò Error cr√≠tico, creando cache de emergencia")
        
        try:
            if self.cache_index_file.exists():
                emergency_backup = self.cache_index_file.with_suffix('.emergency.bak')
                shutil.move(str(self.cache_index_file), str(emergency_backup))
                logger.info(f"üì¶ Archivo problem√°tico movido a: {emergency_backup}")
        except:
            pass
        
        return self._create_empty_cache()
    
    def _save_cache_index(self):
        """Guardar √≠ndice de cache con manejo robusto de Unicode"""
        try:
            if "metadata" not in self.cache_index:
                self.cache_index["metadata"] = {}
            
            self.cache_index["metadata"]["last_modified"] = datetime.now().isoformat()
            self.cache_index["metadata"]["total_entries"] = len(
                self.cache_index.get("entries", {})
            )
            
            temp_file = self.cache_index_file.with_suffix('.tmp')
            
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(
                    self.cache_index, 
                    f, 
                    ensure_ascii=False,
                    indent=2,
                    separators=(',', ': '),
                    default=str
                )
            
            with open(temp_file, 'r', encoding='utf-8') as f:
                json.load(f)
            
            temp_file.replace(self.cache_index_file)
            
            logger.debug("‚úÖ Cache index guardado correctamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando cache index: {e}")
            
            try:
                self._save_emergency_cache()
            except:
                pass
    
    def _save_emergency_cache(self):
        """Guardar versi√≥n simplificada del cache en caso de emergencia"""
        try:
            emergency_cache = {
                "version": "2.0",
                "emergency_save": datetime.now().isoformat(),
                "entries": {},
                "metadata": {"emergency": True}
            }
            
            if "entries" in self.cache_index:
                for key, value in list(self.cache_index["entries"].items())[:100]:
                    try:
                        json.dumps(value)
                        emergency_cache["entries"][key] = value
                    except:
                        continue
            
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                json.dump(emergency_cache, f, ensure_ascii=True)
                
            logger.warning("‚ö†Ô∏è Cache de emergencia guardado")
            
        except Exception as e:
            logger.error(f"‚ùå Fallo cr√≠tico guardando cache de emergencia: {e}")
    
    def get_file_hash(self, file_bytes: bytes) -> str:
        """Generar hash √∫nico para archivo"""
        return hashlib.sha256(file_bytes).hexdigest()[:16]
    
    def get_cached_result(self, file_hash: str, operation: str) -> Optional[Dict[str, Any]]:
        """Obtener resultado cacheado con manejo seguro"""
        cache_key = f"{file_hash}_{operation}"
        entries = self.cache_index.get("entries", {})
        
        if cache_key in entries:
            cache_info = entries[cache_key]
            
            if "filename" in cache_info:
                cache_file = self.cache_dir / cache_info['filename']
                
                if cache_file.exists():
                    cache_info['last_accessed'] = datetime.now().isoformat()
                    self._save_cache_index()
                    return cache_info
                else:
                    logger.warning(f"‚ö†Ô∏è Archivo cache no encontrado: {cache_file}")
                    del entries[cache_key]
                    self._save_cache_index()
        
        return None
    
    def cache_result(self, file_hash: str, operation: str, result: Dict[str, Any], 
                    output_bytes: Optional[bytes] = None) -> str:
        """Cachear resultado con validaci√≥n"""
        cache_key = f"{file_hash}_{operation}"
        cache_filename = f"{cache_key}_{datetime.now().timestamp()}"
        
        try:
            json.dumps(result)
        except (TypeError, ValueError) as e:
            logger.warning(f"‚ö†Ô∏è Resultado no serializable, limpiando: {e}")
            result = self._make_serializable(result)
        
        cache_info = {
            'filename': cache_filename,
            'operation': operation,
            'result': result,
            'created': datetime.now().isoformat(),
            'last_accessed': datetime.now().isoformat(),
            'size_bytes': len(output_bytes) if output_bytes else 0
        }
        
        if output_bytes:
            try:
                cache_file = self.cache_dir / cache_filename
                cache_file.write_bytes(output_bytes)
            except Exception as e:
                logger.error(f"‚ùå Error guardando archivo cache: {e}")
        
        if "entries" not in self.cache_index:
            self.cache_index["entries"] = {}
        
        self.cache_index["entries"][cache_key] = cache_info
        self._save_cache_index()
        
        self._cleanup_cache_if_needed()
        
        return cache_filename
    
    def _make_serializable(self, obj: Any) -> Any:
        """Convertir objeto a formato serializable"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(v) for v in obj]
        elif isinstance(obj, bytes):
            return obj.decode('utf-8', errors='replace')
        elif hasattr(obj, '__dict__'):
            return str(obj)
        else:
            try:
                json.dumps(obj)
                return obj
            except:
                return str(obj)
    
    def _cleanup_cache_if_needed(self):
        """Limpiar cache si excede el tama√±o m√°ximo"""
        entries = self.cache_index.get("entries", {})
        total_size = sum(
            info.get('size_bytes', 0) 
            for info in entries.values()
        )
        
        if total_size > self.max_cache_size:
            logger.info("üßπ Iniciando limpieza de cache...")
            
            sorted_entries = sorted(
                entries.items(),
                key=lambda x: x[1].get('last_accessed', '1970-01-01')
            )
            
            removed_size = 0
            removed_count = 0
            target_remove = total_size - (self.max_cache_size * 0.8)
            
            for cache_key, cache_info in sorted_entries:
                if removed_size >= target_remove:
                    break
                
                if "filename" in cache_info:
                    cache_file = self.cache_dir / cache_info['filename']
                    try:
                        if cache_file.exists():
                            cache_file.unlink()
                    except Exception as e:
                        logger.error(f"‚ùå Error eliminando archivo: {e}")
                
                removed_size += cache_info.get('size_bytes', 0)
                removed_count += 1
                del entries[cache_key]
            
            self._save_cache_index()
            logger.info(
                f"‚úÖ Cache limpiado: {removed_count} entries, "
                f"{removed_size / (1024*1024):.1f}MB liberados"
            )

